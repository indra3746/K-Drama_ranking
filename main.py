import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import traceback
import time # ë”œë ˆì´ë¥¼ ìœ„í•´ í•„ìˆ˜

# 1. í…”ë ˆê·¸ë¨ ì „ì†¡
def send_telegram(text):
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("CHAT_ID")
    if token and chat_id and len(text) > 0:
        try:
            url = f"https://api.telegram.org/bot{token}/sendMessage"
            requests.post(url, json={"chat_id": chat_id, "text": text, "disable_web_page_preview": True})
        except Exception as e:
            print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# 2. ìœ„í‚¤ë°±ê³¼ì—ì„œ ë“œë¼ë§ˆ ì œëª© ìë™ ìˆ˜ì§‘ (Whitelist)
def get_wiki_drama_list():
    print("ğŸ“‹ ìœ„í‚¤ë°±ê³¼ ë“œë¼ë§ˆ DB êµ¬ì¶• ì¤‘...")
    drama_set = set()
    
    # ì‘ë…„ ë§ ~ ì˜¬í•´ ë“œë¼ë§ˆë¥¼ ëª¨ë‘ ì»¤ë²„í•˜ê¸° ìœ„í•´ 2ê°œ ì—°ë„ ê²€ìƒ‰
    urls = [
        "https://ko.wikipedia.org/wiki/2025ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡",
        "https://ko.wikipedia.org/wiki/2026ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for url in urls:
        try:
            print(f"   ì ‘ì†: {url} ...")
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ìœ„í‚¤ë°±ê³¼ 'wikitable' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í‘œë“¤ íƒìƒ‰
            tables = soup.select("table.wikitable")
            for table in tables:
                rows = table.select("tr")
                for row in rows:
                    cols = row.select("td")
                    # ë³´í†µ ì œëª©ì€ ì•ìª½(1~2ë²ˆì§¸) ì¹¸ì— ìœ„ì¹˜í•¨
                    for col in cols[:3]:
                        # 1) <i> íƒœê·¸ (ê¸°ìš¸ì„ê¼´) ì•ˆì— ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” 99% ë“œë¼ë§ˆ ì œëª©
                        italic = col.find("i")
                        if italic:
                            title = italic.get_text(strip=True)
                            drama_set.add(title.replace(" ", ""))
                        
                        # 2) ë§í¬(a) í…ìŠ¤íŠ¸ ì¤‘ ë”°ì˜´í‘œê°€ ìˆê±°ë‚˜ ê¸´ í…ìŠ¤íŠ¸
                        link = col.find("a")
                        if link:
                            t = link.get_text(strip=True)
                            # 'ë³´ê¸°', 'í¸ì§‘' ë“± ì œì™¸
                            if len(t) > 1 and "ë“œë¼ë§ˆ" not in t:
                                drama_set.add(t.replace(" ", ""))
            
            time.sleep(1) # ìœ„í‚¤ ì„œë²„ ë¶€í•˜ ë°©ì§€
            
        except Exception as e:
            print(f"   âš ï¸ ìœ„í‚¤ ì ‘ì† ì‹¤íŒ¨: {e}")

    print(f"âœ… ìœ„í‚¤ë°±ê³¼ DB í™•ë³´ ì™„ë£Œ: ì´ {len(drama_set)}ê°œ ë“œë¼ë§ˆ")
    return drama_set

# 3. ë‹ìŠ¨ì½”ë¦¬ì•„ ë°ì´í„° ìˆ˜ì§‘ (ì„¸ì…˜ ì‚¬ìš© + ë”œë ˆì´)
def fetch_nielsen_data(session, url, type_name):
    print(f"[{type_name}] ë‹ìŠ¨ ì ‘ì† ì‹œë„...")
    
    try:
        # ì ‘ì† ì „ 2ì´ˆ ë”œë ˆì´ (ì‚¬ëŒì¸ ì²™)
        time.sleep(2)
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.nielsenkorea.co.kr/',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        res = session.get(url, headers=headers, timeout=20)
        res.encoding = 'euc-kr' # ì¸ì½”ë”© í•„ìˆ˜
        
        soup = BeautifulSoup(res.text, 'html.parser')
        results = []
        
        table = soup.find("table", class_="ranking_tb")
        
        # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° (ì°¨ë‹¨ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)
        if not table:
            print(f"   âš ï¸ [{type_name}] í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML êµ¬ì¡° í™•ì¸ í•„ìš”.")
            # ë””ë²„ê¹…: í˜¹ì‹œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if "ë¡œê·¸ì¸" in res.text or "Wait" in res.text:
                print("   ğŸš« ì ‘ê·¼ ì œí•œë¨ (Login/Block)")
            return []
            
        rows = table.find_all("tr")
        print(f"   âœ… ë°ì´í„° í…Œì´ë¸” ë°œê²¬ ({len(rows)}í–‰)")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4: continue
            
            try:
                channel = cols[1].get_text(strip=True)
                raw_title = cols[2].get_text(strip=True)
                rating = cols[3].get_text(strip=True)
                
                try:
                    rating_val = float(rating.replace("%", "").strip())
                except:
                    rating_val = 0.0
                
                results.append({
                    "channel": channel,
                    "title": raw_title,
                    "rating": rating,
                    "rating_val": rating_val
                })
            except: continue
            
        return results
        
    except Exception as e:
        print(f"   âŒ [{type_name}] ì—ëŸ¬: {e}")
        return []

# 4. í•„í„°ë§ ë¡œì§ (ìœ„í‚¤ DB vs ë‹ìŠ¨ Raw)
def filter_dramas(nielsen_data, wiki_db):
    filtered = []
    
    for item in nielsen_data:
        raw_title = item['title']
        clean_raw = raw_title.replace(" ", "")
        
        is_match = False
        display_title = raw_title
        
        # 1) ê´„í˜¸ ì•ˆì˜ ì œëª© ì¶”ì¶œ
        # ì˜ˆ: ì¼ì¼ë“œë¼ë§ˆ(ê²°í˜¼í•˜ìë§¹ê½ì•„) -> ê²°í˜¼í•˜ìë§¹ê½ì•„
        match = re.search(r'\((.*?)\)', raw_title)
        extracted = ""
        if match:
            extracted = match.group(1).strip()
            
        # ë§¤ì¹­ ê²€ì‚¬ 1: ê´„í˜¸ ì•ˆ ë‚´ìš©ì´ ìœ„í‚¤ DBì— ìˆëŠ”ê°€?
        if extracted:
            if extracted.replace(" ", "") in wiki_db:
                is_match = True
                display_title = extracted
        
        # ë§¤ì¹­ ê²€ì‚¬ 2: ìœ„í‚¤ ì œëª©ì´ ë‹ìŠ¨ ì›ë³¸ì— í¬í•¨ë˜ëŠ”ê°€?
        if not is_match:
            for wiki_t in wiki_db:
                # ë‹ìŠ¨: "ì£¼ë§ë“œë¼ë§ˆì˜¤ì§•ì–´ê²Œì„2" vs ìœ„í‚¤: "ì˜¤ì§•ì–´ê²Œì„2"
                if wiki_t in clean_raw and len(wiki_t) > 2:
                    is_match = True
                    display_title = wiki_t
                    break
        
        # ë§¤ì¹­ ê²€ì‚¬ 3: (ì•ˆì „ì¥ì¹˜) 'ë“œë¼ë§ˆ', 'ë¯¸ë‹ˆì‹œë¦¬ì¦ˆ' ë‹¨ì–´ í¬í•¨ì‹œ ë¬´ì¡°ê±´ í†µê³¼
        if not is_match:
            if any(k in clean_raw for k in ["ë“œë¼ë§ˆ", "ë¯¸ë‹ˆì‹œë¦¬ì¦ˆ", "ì—°ì†ê·¹"]):
                is_match = True
                if extracted: display_title = extracted # ê¸°ì™•ì´ë©´ ê´„í˜¸ ì•ˆ ë‚´ìš©ìœ¼ë¡œ
        
        if is_match:
            item['display_title'] = display_title
            filtered.append(item)
            
    filtered.sort(key=lambda x: x['rating_val'], reverse=True)
    return filtered

# 5. ë©”ì¸ ì‹¤í–‰
def main():
    try:
        kst_now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
        yesterday = kst_now - datetime.timedelta(days=1)
        days = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        date_str = yesterday.strftime(f"%Y-%m-%d({days[yesterday.weekday()]})")
        
        print(f"--- ì‹¤í–‰ ì‹œì‘ ({date_str} ê¸°ì¤€) ---")
        
        # 1. ìœ„í‚¤ë°±ê³¼ì—ì„œ ë¦¬ìŠ¤íŠ¸ í™•ë³´
        wiki_db = get_wiki_drama_list()
        
        # 2. ë‹ìŠ¨ ë°ì´í„° ìˆ˜ì§‘ (ì„¸ì…˜ í•˜ë‚˜ë¡œ ìœ ì§€)
        session = requests.Session()
        
        url_t = "https://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=1_1&area=00"
        raw_t = fetch_nielsen_data(session, url_t, "ì§€ìƒíŒŒ")
        
        url_c = "https://www.nielsenkorea.co.kr/tv_cable_day.asp?menu=Tit_2&sub_menu=2_1&area=00"
        raw_c = fetch_nielsen_data(session, url_c, "ì¢…í¸/ì¼€ì´ë¸”")
        
        # 3. ë§¤ì¹­ ë° í•„í„°ë§
        final_t = filter_dramas(raw_t, wiki_db)
        final_c_all = filter_dramas(raw_c, wiki_db)
        
        # 4. ì¢…í¸/ì¼€ì´ë¸” ë¶„ë¦¬
        jongpyeon_chs = ["JTBC", "MBN", "TV CHOSUN", "TVì¡°ì„ ", "ì±„ë„A"]
        final_j = []
        final_c = []
        
        for item in final_c_all:
            ch_upper = item['channel'].upper().replace(" ", "")
            if any(j in ch_upper for j in jongpyeon_chs):
                final_j.append(item)
            else:
                final_c.append(item)
        
        final_j.sort(key=lambda x: x['rating_val'], reverse=True)
        final_c.sort(key=lambda x: x['rating_val'], reverse=True)
        
        # 5. ë¦¬í¬íŠ¸ ì‘ì„±
        report = f"ğŸ“º {date_str} ë“œë¼ë§ˆ ì‹œì²­ë¥  ë­í‚¹\n(ë‹ìŠ¨ì½”ë¦¬ì•„ / ì–´ì œ ë°©ì˜ë¶„)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        
        def make_section(title, data):
            txt = f"ğŸ“¡ {title}\n"
            if data:
                for i, item in enumerate(data[:5]):
                    txt += f" {i+1}ìœ„ {item['display_title']} | ({item['channel']}) | {item['rating']}\n"
            else:
                txt += "(ê²°ë°© ë˜ëŠ” ë°ì´í„° ì—†ìŒ)\n"
            return txt + "\n"
        
        report += make_section("ì§€ìƒíŒŒ (Top 5)", final_t)
        report += make_section("ì¢…í¸ (Top 5)", final_j)
        report += make_section("ì¼€ì´ë¸” (Top 5)", final_c)
        
        report += "ğŸ”— ì •ë³´: ë‹ìŠ¨ì½”ë¦¬ì•„ / ìœ„í‚¤ë°±ê³¼"
        
        send_telegram(report)
        print("--- ì „ì†¡ ì™„ë£Œ ---")
        
    except Exception as e:
        err = traceback.format_exc()
        print(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ:\n{err}")
        send_telegram(f"ğŸš¨ ë´‡ ì—ëŸ¬:\n{str(e)}")

if __name__ == "__main__":
    main()
