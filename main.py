import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import traceback
import time
import gzip
import io
from difflib import SequenceMatcher

# 1. í…”ë ˆê·¸ë¨ ì „ì†¡
def send_telegram(text):
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("CHAT_ID")
    if token and chat_id and len(text) > 0:
        try:
            url = f"https://api.telegram.org/bot{token}/sendMessage"
            requests.post(url, json={"chat_id": chat_id, "text": text, "disable_web_page_preview": True})
        except Exception as e:
            print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# ìœ ì‚¬ë„ ê³„ì‚°
def get_similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ì •ê·œí™”
def normalize(text):
    if not text: return ""
    return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', text)

# 2. ìœ„í‚¤ë°±ê³¼ DB êµ¬ì¶•
def get_wiki_drama_list():
    print("ğŸ“‹ ìœ„í‚¤ë°±ê³¼ ë“œë¼ë§ˆ DB êµ¬ì¶• ì¤‘...")
    drama_set = set()
    
    manual_list = [
        "ê²°í˜¼í•˜ìë§¹ê½ì•„", "ì¹œì ˆí•œì„ ì£¼ì”¨", "ìŠ¤ìº”ë“¤", "ì‹¬ì¥ì„í›”ì¹œê²Œì„", 
        "ë‚˜ì˜í•´ë¦¬ì—ê²Œ", "ì¡°ë¦½ì‹ê°€ì¡±", "ì´í˜¼ìˆ™ë ¤ìº í”„", "ë³´ë¬¼ì„¬", 
        "ëª¨í…”ìº˜ë¦¬í¬ë‹ˆì•„", "ëŸ¬ë¸Œë¯¸", "ìŠ¤í”„ë§í”¼ë²„", "ì•„ì´ëŒì•„ì´",
        "ìš©ê°ë¬´ìŒìš©ìˆ˜ì •", "ì„¸ë²ˆì§¸ê²°í˜¼", "ìš°ì•„í•œì œêµ­", "ì€ì• í•˜ëŠ”ë„ì ë‹˜ì•„",
        "ì²«ë²ˆì§¸ë‚¨ì", "ì¹œë°€í•œë¦¬í”Œë¦¬", "í™”ë ¤í•œë‚ ë“¤", "íŒì‚¬ì´í•œì˜"
    ]
    for m in manual_list:
        drama_set.add(normalize(m))
    
    urls = [
        "https://ko.wikipedia.org/wiki/2025ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡",
        "https://ko.wikipedia.org/wiki/2026ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡"
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in urls:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            tables = soup.select("table.wikitable")
            for table in tables:
                rows = table.select("tr")
                for row in rows:
                    cols = row.select("td")
                    for col in cols[:3]:
                        targets = col.find_all(['i', 'a'])
                        for t in targets:
                            text = t.get_text(strip=True)
                            if len(text) > 1 and "ë“œë¼ë§ˆ" not in text:
                                drama_set.add(normalize(text))
        except: pass

    print(f"âœ… ë¹„êµêµ°(Whitelist) í™•ë³´ ì™„ë£Œ: {len(drama_set)}ê°œ")
    return list(drama_set)

# 3. ë‹ìŠ¨ì½”ë¦¬ì•„ ë°ì´í„° ìˆ˜ì§‘ (ì••ì¶• í•´ì œ ë¡œì§ ì¶”ê°€)
def fetch_nielsen_data(session, url, type_name):
    print(f"[{type_name}] ë‹ìŠ¨ ì ‘ì† ì‹œë„: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.nielsenkorea.co.kr/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br' # ì••ì¶• ì§€ì›í•œë‹¤ê³  ëª…ì‹œ
    }
    
    try:
        res = session.get(url, headers=headers, timeout=20)
        
        # [ğŸš¨ í•µì‹¬ ìˆ˜ì •] GZIP ê°•ì œ ì••ì¶• í•´ì œ ì‹œë„
        # ë‹ìŠ¨ ì„œë²„ê°€ í—¤ë” ì—†ì´ ì••ì¶• ë°ì´í„°ë¥¼ ë³´ë‚¼ ë•Œë¥¼ ëŒ€ë¹„í•¨
        html_bytes = res.content
        try:
            # ì• 2ë°”ì´íŠ¸ê°€ GZIP ë§¤ì§ ë„˜ë²„(1f 8b)ì¸ì§€ í™•ì¸í•˜ê±°ë‚˜ ê·¸ëƒ¥ í’€ì–´ë´„
            buf = io.BytesIO(res.content)
            f = gzip.GzipFile(fileobj=buf)
            html_bytes = f.read()
            print(f"   ğŸ”“ [{type_name}] GZIP ì••ì¶• í•´ì œ ì„±ê³µ!")
        except:
            # ì••ì¶•ì´ ì•„ë‹ˆë©´ ì›ë˜ ë°ì´í„° ì‚¬ìš©
            pass
            
        # ê·¸ ë‹¤ìŒ EUC-KR ë””ì½”ë”©
        try:
            html_content = html_bytes.decode('cp949', 'ignore')
        except:
            html_content = html_bytes.decode('euc-kr', 'ignore')
            
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        table = soup.find("table", class_="ranking_tb")
        if not table:
            print(f"   âŒ [{type_name}] í…Œì´ë¸” ëª» ì°¾ìŒ")
            # ë””ë²„ê¹…: ë‚´ìš© ì‚´ì§ ì¶œë ¥
            print(f"   ğŸ“„ ë‚´ìš© ì¼ë¶€: {html_content[:100].strip()}")
            return []
            
        rows = table.find_all("tr")
        print(f"   â„¹ï¸ {len(rows)}í–‰ ë°ì´í„° ë°œê²¬")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4: continue
            
            try:
                channel = cols[1].get_text(strip=True)
                raw_title = cols[2].get_text(strip=True)
                rating = cols[3].get_text(strip=True)
                
                try:
                    rating_val = float(rating.replace("%", "").strip())
                except:
                    rating_val = 0.0
                
                results.append({
                    "channel": channel,
                    "title": raw_title,
                    "rating": rating,
                    "rating_val": rating_val
                })
            except: continue
            
        return results
    except Exception as e:
        print(f"   âŒ [{type_name}] ì ‘ì† ì—ëŸ¬: {e}")
        return []

# 4. í•„í„°ë§ ë¡œì§
def filter_dramas(nielsen_data, wiki_db):
    filtered = []
    
    for item in nielsen_data:
        raw_title = item['title']
        
        # ê´„í˜¸ ì²˜ë¦¬
        match = re.search(r'\((.*?)\)', raw_title)
        extracted = match.group(1).strip() if match else raw_title
        
        target_name = normalize(extracted)
        is_match = False
        display_title = extracted
        
        # ìœ ì‚¬ë„ ë§¤ì¹­
        best_score = 0.0
        for db_title in wiki_db:
            score = get_similarity(target_name, db_title)
            if score > best_score:
                best_score = score
        
        if best_score >= 0.6:
            is_match = True
        
        # í‚¤ì›Œë“œ ë³´ì™„
        if not is_match and any(k in raw_title for k in ["ë“œë¼ë§ˆ", "ë¯¸ë‹ˆì‹œë¦¬ì¦ˆ", "ì—°ì†ê·¹"]):
            is_match = True

        if is_match:
            item['display_title'] = display_title
            item['is_verified'] = True
            filtered.append(item)
    
    # [ì•ˆì „ì¥ì¹˜] í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìƒìœ„ 3ê°œ ê°•ì œ ì¶œë ¥
    if not filtered and nielsen_data:
        print("   âš ï¸ í•„í„°ë§ 0ê°œ -> ìƒìœ„ 3ê°œ ê°•ì œ ì¶œë ¥")
        for item in nielsen_data[:3]:
            item['display_title'] = item['title'] + "(ë¯¸ê²€ì¦)"
            item['is_verified'] = False
            filtered.append(item)

    filtered.sort(key=lambda x: x['rating_val'], reverse=True)
    return filtered

# 5. ë©”ì¸ ì‹¤í–‰
def main():
    try:
        kst_now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
        yesterday = kst_now - datetime.timedelta(days=1)
        days = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        date_str = yesterday.strftime(f"%Y-%m-%d({days[yesterday.weekday()]})")
        
        print(f"--- ì‹¤í–‰ ì‹œì‘ ({date_str} ê¸°ì¤€) ---")
        
        wiki_db = get_wiki_drama_list()
        
        session = requests.Session()
        
        # 1. ì§€ìƒíŒŒ
        url_t = "https://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=1_1&area=00"
        raw_t = fetch_nielsen_data(session, url_t, "ì§€ìƒíŒŒ")
        final_t = filter_dramas(raw_t, wiki_db)
        
        time.sleep(3)
        
        # 2. ì¢…í¸/ì¼€ì´ë¸”
        url_c = "https://www.nielsenkorea.co.kr/tv_cable_day.asp?menu=Tit_2&sub_menu=2_1&area=00"
        raw_c = fetch_nielsen_data(session, url_c, "ì¢…í¸/ì¼€ì´ë¸”")
        final_c_all = filter_dramas(raw_c, wiki_db)
        
        # ë¶„ë¥˜
        jongpyeon_chs = ["JTBC", "MBN", "TV CHOSUN", "TVì¡°ì„ ", "ì±„ë„A"]
        final_j = []
        final_c = []
        
        for item in final_c_all:
            ch_norm = normalize(item['channel']).upper()
            if any(normalize(j).upper() in ch_norm for j in jongpyeon_chs):
                final_j.append(item)
            else:
                final_c.append(item)
        
        final_j.sort(key=lambda x: x['rating_val'], reverse=True)
        final_c.sort(key=lambda x: x['rating_val'], reverse=True)
        
        # ë¦¬í¬íŠ¸ ì‘ì„±
        report = f"ğŸ“º {date_str} ë“œë¼ë§ˆ ì‹œì²­ë¥  ë­í‚¹\n(ë‹ìŠ¨ì½”ë¦¬ì•„ / ì–´ì œ ë°©ì˜ë¶„)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        
        def make_section(title, data):
            txt = f"ğŸ“¡ {title}\n"
            if data:
                for i, item in enumerate(data[:5]):
                    mark = "" if item.get('is_verified') else "â“"
                    txt += f" {i+1}ìœ„ {mark}{item['display_title']} | ({item['channel']}) | {item['rating']}\n"
            else:
                txt += "(ê²°ë°© ë˜ëŠ” ë°ì´í„° ì—†ìŒ)\n"
            return txt + "\n"
        
        report += make_section("ì§€ìƒíŒŒ", final_t)
        report += make_section("ì¢…í¸", final_j)
        report += make_section("ì¼€ì´ë¸”", final_c)
        
        report += "ğŸ”— ì •ë³´: ë‹ìŠ¨ì½”ë¦¬ì•„"
        
        send_telegram(report)
        print("--- ì „ì†¡ ì™„ë£Œ ---")
        
    except Exception as e:
        err = traceback.format_exc()
        print(f"ğŸ”¥ ì—ëŸ¬: {err}")
        send_telegram(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
