import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import traceback
import time
import gzip
import io
from difflib import SequenceMatcher

# 1. í…”ë ˆê·¸ë¨ ì „ì†¡
def send_telegram(text):
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("CHAT_ID")
    if token and chat_id and len(text) > 0:
        try:
            url = f"https://api.telegram.org/bot{token}/sendMessage"
            requests.post(url, json={"chat_id": chat_id, "text": text, "disable_web_page_preview": True}) 
        except Exception as e:
            print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# ìœ ì‚¬ë„ ê³„ì‚°
def get_similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ì •ê·œí™”
def normalize(text):
    if not text: return ""
    return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', text)

# ì œëª© ì •ì œ
def clean_title_text(text):
    text = re.sub(r'\(.*?\)', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    return text.strip()

# ë‹ìŠ¨ ì‘ë‹µ ë³µêµ¬
def get_decoded_html(response):
    content = response.content
    if len(content) > 2 and content[:2] == b'\x1f\x8b':
        try:
            buf = io.BytesIO(content)
            with gzip.GzipFile(fileobj=buf) as f:
                content = f.read()
        except: pass
    try:
        return content.decode('cp949')
    except:
        try:
            return content.decode('euc-kr')
        except:
            return content.decode('utf-8', 'ignore')

# [í•µì‹¬ ì—…ê·¸ë ˆì´ë“œ] ìœ„í‚¤ë°±ê³¼ì—ì„œ 'ìš”ì¼ ì •ë³´'ê¹Œì§€ ê°™ì´ ê¸ì–´ì˜´
def get_wiki_drama_db():
    print("ğŸ“‹ ìœ„í‚¤ë°±ê³¼ ë“œë¼ë§ˆ DB(ìš”ì¼ í¬í•¨) êµ¬ì¶• ì¤‘...")
    
    # êµ¬ì¡°: {'ë“œë¼ë§ˆì œëª©ì •ê·œí™”': [0, 1]}  (0=ì›”, 1=í™” ...)
    drama_schedule = {}
    
    # 1. ìˆ˜ë™ ë¦¬ìŠ¤íŠ¸ (ìš”ì¼ì„ ëª¨ë¥´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ [])
    # í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— íŠ¹ì • ë“œë¼ë§ˆ ìš”ì¼ì„ ì§€ì •í•  ìˆ˜ë„ ìˆìŒ
    manual_list = [
        "ê²°í˜¼í•˜ìë§¹ê½ì•„", "ì¹œì ˆí•œì„ ì£¼ì”¨", "ìŠ¤ìº”ë“¤", "ì‹¬ì¥ì„í›”ì¹œê²Œì„", 
        "ë‚˜ì˜í•´ë¦¬ì—ê²Œ", "ì¡°ë¦½ì‹ê°€ì¡±", "ì´í˜¼ìˆ™ë ¤ìº í”„", "ë³´ë¬¼ì„¬", 
        "ëª¨í…”ìº˜ë¦¬í¬ë‹ˆì•„", "ëŸ¬ë¸Œë¯¸", "ìŠ¤í”„ë§í”¼ë²„", "ì•„ì´ëŒì•„ì´",
        "ìš©ê°ë¬´ìŒìš©ìˆ˜ì •", "ì„¸ë²ˆì§¸ê²°í˜¼", "ìš°ì•„í•œì œêµ­", "ì€ì• í•˜ëŠ”ë„ì ë‹˜ì•„",
        "ì²«ë²ˆì§¸ë‚¨ì", "ì¹œë°€í•œë¦¬í”Œë¦¬", "í™”ë ¤í•œë‚ ë“¤", "íŒì‚¬ì´í•œì˜",
        "ë§ˆë¦¬ì™€ë³„ë‚œì•„ë¹ ë“¤", "êµ¿ë³´ì´", "ë„‰ì˜¤í”„", "íŠ¸ë¦¬ê±°", "í•˜ì´í¼ë‚˜ì´í”„"
    ]
    for m in manual_list:
        drama_schedule[normalize(m)] = [] # ìš”ì¼ ëª¨ë¦„ (íƒœê·¸ì—ë§Œ ì˜ì¡´)
    
    urls = [
        "https://ko.wikipedia.org/wiki/2025ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡",
        "https://ko.wikipedia.org/wiki/2026ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡"
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in urls:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # [Smart Parsing] í—¤ë”(ìš”ì¼)ì™€ í…Œì´ë¸”ì„ ìˆœì„œëŒ€ë¡œ ì½ìŒ
            elements = soup.find_all(['h2', 'h3', 'h4', 'table'])
            current_days = [] # í˜„ì¬ ì½ê³  ìˆëŠ” ì„¹ì…˜ì˜ ìš”ì¼
            
            for el in elements:
                # 1) í—¤ë”ì—ì„œ ìš”ì¼ ê°ì§€
                if el.name in ['h2', 'h3', 'h4']:
                    text = el.get_text()
                    if "ì›”í™”" in text: current_days = [0, 1] # ì›”, í™”
                    elif "ìˆ˜ëª©" in text: current_days = [2, 3] # ìˆ˜, ëª©
                    elif "ê¸ˆí† " in text: current_days = [4, 5] # ê¸ˆ, í† 
                    elif "ì£¼ë§" in text or "í† ì¼" in text: current_days = [5, 6] # í† , ì¼
                    elif "ì¼ì¼" in text: current_days = [0, 1, 2, 3, 4] # ì›”~ê¸ˆ
                    else: pass # ê¸°íƒ€ ì„¹ì…˜ì€ ìš”ì¼ ìœ ì§€ í˜¹ì€ ì´ˆê¸°í™” (ì—¬ê¸°ì„  ìœ ì§€)
                
                # 2) í…Œì´ë¸”ì—ì„œ ì œëª© ì¶”ì¶œ í›„ í˜„ì¬ ìš”ì¼ í• ë‹¹
                elif el.name == 'table' and 'wikitable' in el.get('class', []):
                    rows = el.select("tr")
                    for row in rows:
                        cols = row.select("td")
                        for col in cols[:3]: # ì•ìª½ ì»¬ëŸ¼ì—ì„œ ì œëª© ì°¾ê¸°
                            targets = col.find_all(['i', 'a'])
                            for t in targets:
                                text = t.get_text(strip=True)
                                if len(text) > 1 and "ë“œë¼ë§ˆ" not in text:
                                    norm_title = normalize(text)
                                    # ì´ë¯¸ ìˆ˜ë™ìœ¼ë¡œ ë„£ì€ ê±´ ë®ì–´ì“°ì§€ ì•ŠìŒ (í˜¹ì€ ë®ì–´ì¨ì„œ ìš”ì¼ ì—…ë°ì´íŠ¸)
                                    if norm_title not in drama_schedule or not drama_schedule[norm_title]:
                                        drama_schedule[norm_title] = current_days
        except: pass

    print(f"âœ… ë¹„êµêµ° í™•ë³´ ì™„ë£Œ: {len(drama_schedule)}ê°œ")
    return drama_schedule

# 3. ë‹ìŠ¨ì½”ë¦¬ì•„ ë°ì´í„° ìˆ˜ì§‘
def fetch_nielsen_data(session, url, type_name):
    print(f"[{type_name}] ì ‘ì†: {url}")
    headers = {
        'User-Agent': 'Mozilla/5.0',
        'Referer': 'https://www.nielsenkorea.co.kr/',
        'Accept-Encoding': 'gzip, deflate'
    }
    
    try:
        res = session.get(url, headers=headers, timeout=20)
        html_content = get_decoded_html(res) 
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        table = soup.find("table", class_="ranking_tb")
        if not table: return []
            
        rows = table.find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4: continue
            
            try:
                channel = cols[1].get_text(strip=True)
                raw_title = cols[2].get_text(strip=True)
                rating = cols[3].get_text(strip=True)
                
                if "ì‹œì²­ë¥ " in rating or "í”„ë¡œê·¸ë¨" in raw_title: continue
                
                try: rating_val = float(rating.replace("%", "").strip())
                except: rating_val = 0.0
                
                results.append({
                    "channel": channel,
                    "title": raw_title,
                    "rating": rating,
                    "rating_val": rating_val
                })
            except: continue
        return results
    except: return []

# 4. í•„í„°ë§ ë¡œì§ (ìš”ì¼ ì²´í¬ ì¶”ê°€)
def filter_dramas(nielsen_data, wiki_db, yesterday_weekday):
    filtered = []
    
    for item in nielsen_data:
        raw_title = item['title']
        
        # 1. ì¬ë°©ì†¡ ì—¬ë¶€ íŒë‹¨ (ìš°ì„ ìˆœìœ„: íƒœê·¸ > ìš”ì¼ ë¶ˆì¼ì¹˜)
        is_rerun = False
        
        # A. íƒœê·¸ ì²´í¬
        if "<ì¬>" in raw_title or "(ì¬)" in raw_title:
            is_rerun = True
            
        # ë§¤ì¹­ìš© ì •ì œ
        match = re.search(r'\((.*?)\)', raw_title)
        extracted = match.group(1).strip() if match else raw_title
        extracted = re.sub(r'<.*?>', '', extracted)
        target_name = normalize(extracted)
        
        # B. ìœ ì‚¬ë„ ë§¤ì¹­ ë° ìŠ¤ì¼€ì¤„ í™•ì¸
        is_match = False
        best_score = 0.0
        matched_wiki_days = [] # ë§¤ì¹­ëœ ë“œë¼ë§ˆì˜ ë°©ì˜ ìš”ì¼
        
        for db_title, days in wiki_db.items():
            score = get_similarity(target_name, db_title)
            if score > best_score:
                best_score = score
                matched_wiki_days = days
        
        if best_score >= 0.6:
            is_match = True
            
        # í‚¤ì›Œë“œ ë³´ì™„
        if not is_match and any(k in raw_title for k in ["ë“œë¼ë§ˆ", "ë¯¸ë‹ˆì‹œë¦¬ì¦ˆ", "ì—°ì†ê·¹"]):
            is_match = True

        # [í•µì‹¬] ìš”ì¼ ë¶ˆì¼ì¹˜ ì²´í¬
        # íƒœê·¸ê°€ ì—†ì—ˆë”ë¼ë„, ìœ„í‚¤ì— ë“±ë¡ëœ ìš”ì¼ê³¼ ì–´ì œ ìš”ì¼ì´ ë‹¤ë¥´ë©´ ì¬ë°©ì†¡ ì·¨ê¸‰
        # (ë‹¨, ìš”ì¼ ì •ë³´ê°€ ë¹„ì–´ìˆìœ¼ë©´ íŒë‹¨ ì•ˆ í•¨)
        if is_match and not is_rerun and matched_wiki_days:
            if yesterday_weekday not in matched_wiki_days:
                is_rerun = True
                # print(f"   ğŸ’¡ ì¬ë°©ì†¡ ê°ì§€(ìš”ì¼ë‹¤ë¦„): {raw_title} (ì–´ì œ:{yesterday_weekday} vs ë°©ì†¡:{matched_wiki_days})")

        if is_match:
            display_title = clean_title_text(raw_title)
            if match: display_title = clean_title_text(match.group(1))
            
            # ì¬ë°©ì†¡ì´ë©´ ì•ì— í‘œì‹œ
            if is_rerun:
                display_title = "(ì¬) " + display_title.replace("(ì¬)", "").strip()
            
            item['display_title'] = display_title
            item['is_verified'] = True
            filtered.append(item)
    
    filtered.sort(key=lambda x: x['rating_val'], reverse=True)
    return filtered

# 5. ë©”ì¸ ì‹¤í–‰
def main():
    try:
        kst_now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
        yesterday = kst_now - datetime.timedelta(days=1)
        # ìš”ì¼ ìˆ«ì (0:ì›”, 1:í™” ... 6:ì¼)
        yesterday_weekday = yesterday.weekday()
        
        days_str = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        date_str = yesterday.strftime(f"%Y-%m-%d({days_str[yesterday_weekday]})")
        
        print(f"--- ì‹¤í–‰ ì‹œì‘ ({date_str} / ìˆ˜ë„ê¶Œ) ---")
        
        # DB êµ¬ì¶• (ìš”ì¼ ì •ë³´ í¬í•¨)
        wiki_db = get_wiki_drama_db()
        
        session = requests.Session()
        
        # URL (ìˆ˜ë„ê¶Œ area=01)
        url_t = "https://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=1_1&area=01"
        url_j = "https://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=2_1&area=01"
        url_c = "https://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=3_1&area=01"
        
        # ìˆ˜ì§‘ ë° í•„í„°ë§ (yesterday_weekday ì „ë‹¬)
        final_t = filter_dramas(fetch_nielsen_data(session, url_t, "ì§€ìƒíŒŒ"), wiki_db, yesterday_weekday)
        time.sleep(1)
        final_j = filter_dramas(fetch_nielsen_data(session, url_j, "ì¢…í¸"), wiki_db, yesterday_weekday)
        time.sleep(1)
        final_c = filter_dramas(fetch_nielsen_data(session, url_c, "ì¼€ì´ë¸”"), wiki_db, yesterday_weekday)
        
        # ë¦¬í¬íŠ¸ ì‘ì„±
        report = f"ğŸ“º {date_str} ë“œë¼ë§ˆ ì‹œì²­ë¥  ë­í‚¹\n(ë‹ìŠ¨ì½”ë¦¬ì•„ / ìˆ˜ë„ê¶Œ / ì–´ì œ ë°©ì˜ë¶„)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        
        def make_section(title, data):
            txt = f"ğŸ“¡ {title}\n"
            if data:
                for i, item in enumerate(data[:5]):
                    txt += f" {i+1}ìœ„ {item['display_title']} | ({item['channel']}) | {item['rating']}\n"
            else:
                txt += "(ê²°ë°© ë˜ëŠ” ë°ì´í„° ì—†ìŒ)\n"
            return txt + "\n"
        
        report += make_section("ì§€ìƒíŒŒ", final_t)
        report += make_section("ì¢…í¸", final_j)
        report += make_section("ì¼€ì´ë¸”", final_c)
        
        report += "ğŸ”— ì •ë³´: ë‹ìŠ¨ì½”ë¦¬ì•„\nhttps://www.nielsenkorea.co.kr/tv_terrestrial_day.asp?menu=Tit_1&sub_menu=1_1&area=01"
        
        send_telegram(report)
        print("--- ì „ì†¡ ì™„ë£Œ ---")
        
    except Exception as e:
        err = traceback.format_exc()
        print(f"ğŸ”¥ ì—ëŸ¬: {err}")
        send_telegram(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
