import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import traceback
import time
from difflib import SequenceMatcher # [í•µì‹¬] ìœ ì‚¬ë„ ë¹„êµ ë„êµ¬

# 1. í…”ë ˆê·¸ë¨ ì „ì†¡
def send_telegram(text):
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("CHAT_ID")
    if token and chat_id and len(text) > 0:
        try:
            url = f"https://api.telegram.org/bot{token}/sendMessage"
            requests.post(url, json={"chat_id": chat_id, "text": text, "disable_web_page_preview": True})
        except Exception as e:
            print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")

# [í•µì‹¬ í•¨ìˆ˜] ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)
def get_similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ë¬¸ìì—´ ì •ê·œí™” (ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë¹„êµìš©)
def normalize(text):
    if not text: return ""
    return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', text)

# 2. ìœ„í‚¤ë°±ê³¼ DB êµ¬ì¶• (Whitelist)
def get_wiki_drama_list():
    print("ğŸ“‹ ìœ„í‚¤ë°±ê³¼ ë“œë¼ë§ˆ DB êµ¬ì¶• ì¤‘...")
    drama_set = set()
    
    # [ë¹„ìƒìš© ìˆ˜ë™ ë¦¬ìŠ¤íŠ¸] ìœ„í‚¤ì— ì—†ì–´ë„ ì´ê±´ ê¼­ ì±™ê²¨ë¼
    manual_list = [
        "ê²°í˜¼í•˜ìë§¹ê½ì•„", "ì¹œì ˆí•œì„ ì£¼ì”¨", "ìŠ¤ìº”ë“¤", "ì‹¬ì¥ì„í›”ì¹œê²Œì„", 
        "ë‚˜ì˜í•´ë¦¬ì—ê²Œ", "ì¡°ë¦½ì‹ê°€ì¡±", "ì´í˜¼ìˆ™ë ¤ìº í”„", "ë³´ë¬¼ì„¬", 
        "ëª¨í…”ìº˜ë¦¬í¬ë‹ˆì•„", "ëŸ¬ë¸Œë¯¸", "ìŠ¤í”„ë§í”¼ë²„", "ì•„ì´ëŒì•„ì´",
        "ìš©ê°ë¬´ìŒìš©ìˆ˜ì •", "ì„¸ë²ˆì§¸ê²°í˜¼", "ìš°ì•„í•œì œêµ­"
    ]
    for m in manual_list:
        drama_set.add(normalize(m))
    
    # ìœ„í‚¤ë°±ê³¼ í¬ë¡¤ë§
    urls = [
        "https://ko.wikipedia.org/wiki/2025ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡",
        "https://ko.wikipedia.org/wiki/2026ë…„_ëŒ€í•œë¯¼êµ­ì˜_í…”ë ˆë¹„ì „_ë“œë¼ë§ˆ_ëª©ë¡"
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in urls:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            tables = soup.select("table.wikitable")
            for table in tables:
                rows = table.select("tr")
                for row in rows:
                    cols = row.select("td")
                    for col in cols[:3]:
                        targets = col.find_all(['i', 'a'])
                        for t in targets:
                            text = t.get_text(strip=True)
                            # 'ë³´ê¸°', 'ë“œë¼ë§ˆ' ë“± ì œì™¸í•˜ê³  ì œëª©ë§Œ
                            if len(text) > 1 and "ë“œë¼ë§ˆ" not in text:
                                drama_set.add(normalize(text))
        except: pass

    print(f"âœ… ë¹„êµêµ°(Whitelist) í™•ë³´ ì™„ë£Œ: {len(drama_set)}ê°œ")
    return list(drama_set) # ìœ ì‚¬ë„ ë¹„êµë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

# 3. ë‹ìŠ¨ì½”ë¦¬ì•„ ë°ì´í„° ìˆ˜ì§‘
def fetch_nielsen_data(session, url, type_name):
    print(f"[{type_name}] ë‹ìŠ¨ ì ‘ì† ì‹œë„: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': 'https://www.nielsenkorea.co.kr/',
        'Cache-Control': 'no-cache'
    }
    
    try:
        res = session.get(url, headers=headers, timeout=20)
        res.encoding = 'euc-kr' 
        
        soup = BeautifulSoup(res.text, 'html.parser')
        results = []
        
        table = soup.find("table", class_="ranking_tb")
        if not table:
            print(f"   âŒ [{type_name}] í…Œì´ë¸” ëª» ì°¾ìŒ")
            return []
            
        rows = table.find_all("tr")
        print(f"   â„¹ï¸ {len(rows)}í–‰ ë°ì´í„° ë°œê²¬")
        
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4: continue
            
            try:
                channel = cols[1].get_text(strip=True)
                raw_title = cols[2].get_text(strip=True)
                rating = cols[3].get_text(strip=True)
                
                try:
                    rating_val = float(rating.replace("%", "").strip())
                except:
                    rating_val = 0.0
                
                results.append({
                    "channel": channel,
                    "title": raw_title,
                    "rating": rating,
                    "rating_val": rating_val
                })
            except: continue
            
        return results
    except Exception as e:
        print(f"   âŒ [{type_name}] ì ‘ì† ì—ëŸ¬: {e}")
        return []

# 4. í•„í„°ë§ ë¡œì§ (ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­)
def filter_dramas(nielsen_data, wiki_db):
    filtered = []
    
    for item in nielsen_data:
        raw_title = item['title']
        
        # 1. ê´„í˜¸ ì¶”ì¶œ: "ì¼ì¼ë“œë¼ë§ˆ(ê²°í˜¼í•˜ì ë§¹ê½ì•„)" -> "ê²°í˜¼í•˜ì ë§¹ê½ì•„"
        match = re.search(r'\((.*?)\)', raw_title)
        extracted = match.group(1).strip() if match else raw_title
        
        # ë¹„êµë¥¼ ìœ„í•´ ì •ê·œí™”(ê³µë°±ì œê±°)
        target_name = normalize(extracted)
        
        is_match = False
        display_title = extracted
        
        # [ìœ ì‚¬ë„ ë§¤ì¹­ ì‹œì‘]
        # Whitelist(ìœ„í‚¤DB)ì— ìˆëŠ” ì œëª©ë“¤ê³¼ í•˜ë‚˜ì”© ë¹„êµí•´ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì°¾ìŒ
        best_score = 0.0
        
        for db_title in wiki_
